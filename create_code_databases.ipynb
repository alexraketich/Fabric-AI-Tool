{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5611f41-b3f3-4414-84b9-7c47754f4bd4",
   "metadata": {},
   "source": [
    "# Create Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ef257-4bfc-4852-aa45-71c03cb03cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import nbformat\n",
    "import json\n",
    "\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614ea19-6cd2-48a3-a708-08e916e49955",
   "metadata": {},
   "source": [
    "## (Optional) Pull code from Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff8aa3-05cb-4ef0-b922-e9456da0592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/fabric-testbed/jupyter-examples.git\n",
    "! git clone https://github.com/fabric-testbed/teaching-materials.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6db231",
   "metadata": {},
   "source": [
    "## Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_url(local_directory, full_path, base_url=\"https://github.com/fabric-testbed/jupyter-examples/blob/main\"):\n",
    "    \"\"\"\n",
    "    Constructs the git url for each notebook\n",
    "    Arguments: \n",
    "        - local_directory: the directory where files are stored locally\n",
    "        - full_path: full path to the file locally \n",
    "        - base_url: base git url for the repository \n",
    "    Returns: \n",
    "        - url: the final git url for the given file \n",
    "    \"\"\"\n",
    "    url_path = full_path.replace(local_directory, '')\n",
    "    url = base_url + url_path\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615983cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_notebooks(directory) -> list:\n",
    "    \"\"\"\n",
    "    Recursively finds all Jupyter Notebooks in the give directory\n",
    "    Arguments:\n",
    "        - directory: the directory to be traversed to find notebooks to process\n",
    "    Returns: \n",
    "        - notebooks: a list of the paths to the notebooks(.ipynb files) found \n",
    "    \"\"\"\n",
    "    notebooks = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        # print(f\"This is the root directory: {root}\")\n",
    "        for file in files:\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                # Load the notebook\n",
    "                nb = nbformat.read(full_path, as_version=4)\n",
    "                # Add url as custom metadata\n",
    "                nb.metadata[\"url\"] = construct_url(directory, full_path)\n",
    "                # Save back\n",
    "                nbformat.write(nb, full_path)\n",
    "                # add path to the notebooks list\n",
    "                notebooks.append(full_path)\n",
    "    return notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a4961-fd4d-4415-bd0d-4877fe9f7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_notebook_to_markdown(notebook_path, output_dir) -> str:\n",
    "    \"\"\"\n",
    "    Converts each Jupyter Notebook to Markdown using nbconvert\n",
    "    Arguments: \n",
    "        - notebook_path: path to a single notebook file that is to be converted\n",
    "        - output_dir: path to directory that will store the converted files\n",
    "    Returns:\n",
    "        - markdown_path: a path to the converted file\n",
    "        - None: returns None if markdown_path doesn't exist\n",
    "    \"\"\"\n",
    "    # Construct a path to the converted file by replacing .ipynb with .md\n",
    "    markdown_path = os.path.join(output_dir, os.path.basename(notebook_path).replace('.ipynb', '.md'))\n",
    "    # Convert using nbconvert\n",
    "    command = f'jupyter nbconvert --to markdown \"{notebook_path}\" --output-dir {output_dir}'\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "    # Return the path if it exists and None if it doesn't \n",
    "    return markdown_path if os.path.exists(markdown_path) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_notebook_to_py(notebook_path, output_dir) -> str:\n",
    "    \"\"\"\n",
    "    Converts each Jupyter Notebook to Python using nbconvert\n",
    "    Arguments: \n",
    "        - notebook_path: path to a single notebook file that is to be converted\n",
    "        - output_dir: path to directory that will store the converted files\n",
    "    Returns:\n",
    "        - py_path: a path to the converted file\n",
    "        - None: returns None if py_path doesn't exist\n",
    "    \"\"\"\n",
    "    # Construct a path to the converted file by replacing .ipynb with .py\n",
    "    py_path = os.path.join(output_dir, os.path.basename(notebook_path).replace('.ipynb', '.py'))\n",
    "\n",
    "    # Convert the .ipynb file to .py file using nbconvert\n",
    "    command = f'jupyter nbconvert --to script \"{notebook_path}\" --output-dir {output_dir}'\n",
    "    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    # Only go through this if py_path exists(i.e successful conversion)\n",
    "    if os.path.exists(py_path):\n",
    "        # Get the url from notebook's metadata \n",
    "        with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        url = nb.metadata.get(\"url\")\n",
    "\n",
    "        # For debugging\n",
    "        # print(f\"This is the url retreieved from notebook metadata: {url}\")\n",
    "        \n",
    "        if url:\n",
    "            metadata_comment = \"# === Notebook Metadata ===\\n\"\n",
    "            metadata_comment += f\"# url: {url}\\n\"\n",
    "            metadata_comment += \"# =========================\\n\\n\"\n",
    "\n",
    "            # For debugging\n",
    "            # print(f\"This is the metadata comment to be added: {metadata_comment}\")\n",
    "\n",
    "            # Append url as metadata dictionary to .py file \n",
    "            with open(py_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(metadata_comment)\n",
    "\n",
    "    # Return the path if it exists and None if it doesn't \n",
    "    return py_path if os.path.exists(py_path) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notebooks(notebook_directory, output_dir, doc_type):\n",
    "    \"\"\"\n",
    "    Process all notebooks and convert them as needed\n",
    "    Arguments: \n",
    "        - notebook_directory: directory where notebooks for the vectorstore are located\n",
    "        - output_dir: directory where converted files will be stored\n",
    "        - doc_type: type of file that notebooks should be converted to \n",
    "    \"\"\"\n",
    "\n",
    "    notebooks = find_notebooks(notebook_directory)\n",
    "\n",
    "    # Process each notebook, one at a time\n",
    "    for i, notebook_path in enumerate(notebooks):\n",
    "        if doc_type == \"markdown_whole_page\":\n",
    "            # Convert notebook to Markdown\n",
    "            page_path = convert_notebook_to_markdown(notebook_path, output_dir)\n",
    "            if page_path is None:\n",
    "                print(f\"Failed to convert {notebook_path} to Markdown.\")\n",
    "                continue\n",
    "\n",
    "        elif doc_type == \"py_whole_page\":\n",
    "            # Convert notebook to Python \n",
    "            page_path = convert_notebook_to_py(notebook_path, output_dir)\n",
    "            if page_path is None:\n",
    "                print(f\"Failed to convert {notebook_path} to Python script.\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4b900",
   "metadata": {},
   "source": [
    "## Loading, Splitting and Creating Vectorstore functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** This function currently doesn't work as intended. The urls associated with each file \n",
    "# are mismatched and don't point to the right files ***\n",
    "def load_markdown_content(markdown_dir) -> list:\n",
    "    \"\"\"\n",
    "    Load entire content of each file in the given directory into one document each \n",
    "    Arguments: \n",
    "        - markdown_dir: directory that has the files to load\n",
    "    Returns:\n",
    "        - documents: a list of the documents created from all the files in the directory \n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # Go through each file in the directory \n",
    "    for i, filename in enumerate(os.listdir(markdown_dir)):\n",
    "        # Construct the file path for the file\n",
    "        filepath = os.path.join(markdown_dir, filename)\n",
    "\n",
    "        # With the file open, read contents and create document \n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "            # Add file name as a header\n",
    "            extended_markdown_content = f\"# {filename} \\n\\n{content}\" \n",
    "\n",
    "            # Convert to a Document and add to list\n",
    "            # Include file path as metadata \n",
    "            document = Document(page_content=extended_markdown_content, \n",
    "                                metadata={\"source\": filepath}, id=i)\n",
    "            documents.append(document)\n",
    "    \n",
    "    return documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f259925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_py_content(py_dir) -> list:\n",
    "    \"\"\"\n",
    "    Load entire content of each file in the given directory into one document each \n",
    "    Arguments: \n",
    "        - py_dir: directory that has the files to load\n",
    "    Returns:\n",
    "        - documents: a list of the documents created from all the files in the directory \n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Go through each file in the directory \n",
    "    for i, filename in enumerate(os.listdir(py_dir)):\n",
    "        filepath = os.path.join(py_dir, filename)\n",
    "\n",
    "        # With the file open, read contents and create document \n",
    "        with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "            lines = content.splitlines()\n",
    "\n",
    "            # Get url from metadata comment\n",
    "            url = \"\"\n",
    "            for line in lines:\n",
    "                if line.startswith(\"# url: \"):\n",
    "                    url = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "            # For debugging\n",
    "            # print(f\"This is the url found while loading the contents: {url}\")\n",
    "\n",
    "            # Add file name as a header\n",
    "            extended_py_content = f\"{content[:39]} #{filename}\\n\\n {content[39:]}\"\n",
    "\n",
    "\n",
    "            # Remove lines that match \"# In[ ]:\"\n",
    "            cleaned_py_content = re.sub(r'^\\s*# In\\[\\s*\\d*\\s*\\]:\\s*\\n?', '',\n",
    "                                        extended_py_content, flags=re.MULTILINE)\n",
    "\n",
    "            # Convert to a Document and add to list\n",
    "            # Include file path as metadata \n",
    "            document = Document(page_content=cleaned_py_content,\n",
    "                                metadata={\"source\": filepath, \"url\": url}, id=i)\n",
    "            documents.append(document)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d7a29-cae1-4942-89e9-8a55aec50b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(documents, database_loc, embedding=\"all-mpnet-base-v2\"):\n",
    "    \"\"\"\n",
    "    Creates vector store at the given location using the given documents and embeddings\n",
    "    Arguments: \n",
    "        - documents: documents to be loaded into the vectorstore\n",
    "        - database_loc: the location to store the vectorestore\n",
    "        - embedding: the embedding model to use in creating the vectorstore\n",
    "    \"\"\"\n",
    "   \n",
    "    # Initialize the embedding model and the vector store\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding)\n",
    "    vector_store = Chroma(embedding_function=embedding_model,\n",
    "                          persist_directory=database_loc)\n",
    "\n",
    "    # Store the document in the vector store\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    # Add all documents to the vector store, associated with their unique ids. \n",
    "    vector_store.add_documents(documents, ids=uuids)\n",
    "    \n",
    "    print(\"All notebooks have been processed and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a9141",
   "metadata": {},
   "source": [
    "## Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c83752-4c06-4f5d-86e1-140b9cd1fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_db_pipeline(notebook_directory, output_dir, database_loc,\n",
    "                    doc_type, embedding=\"all-mpnet-base-v2\"):\n",
    "    \"\"\"\n",
    "    Pre-processes the notebooks in the provided notebook_directory and\n",
    "    load all the markdown/python content from them. Finally, creates the\n",
    "    vectorDB\n",
    "    \"\"\"\n",
    "    # Call process function to pre-process notebooks \n",
    "    process_notebooks(notebook_directory, output_dir, doc_type)\n",
    "    \n",
    "    # Based on doc type, load the content from the found and converted notebooks \n",
    "    if doc_type == \"markdown_whole_page\":\n",
    "        documents = load_markdown_content(output_dir)\n",
    "\n",
    "    elif doc_type == \"py_whole_page\":\n",
    "        documents = load_py_content(output_dir)\n",
    "\n",
    "    # Call create vectorstore function to perform embedding and create vectostore \n",
    "    create_vectorstore(documents, database_loc, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663762e-5a08-4704-98df-53b8b87158c4",
   "metadata": {},
   "source": [
    "# Set locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203cb9a-def2-4601-b2d3-606d1ffffef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_directory = \"/root/dir/for/notebooks\"\n",
    "output_files_dir = \"/root/dir/for/converted/files\"\n",
    "database_loc = \"/path/to/vectorDB/dir\"\n",
    "doc_type = \"py_whole_page\" # or \"markdown_whole_page\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095335d9-5756-4ca0-b577-eac019bc4949",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e10ecd-7ad3-48cc-8206-0fdad5668db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm converted files have a place to go\n",
    "os.makedirs(output_files_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2337f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_db_pipeline(notebook_directory, output_files_dir, database_loc, doc_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
